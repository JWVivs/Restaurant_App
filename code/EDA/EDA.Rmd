---
title: "EDA"
author: "John Viviani"
date: "5/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

c("dplyr",
  "ggplot2",
  "tidyr",
  "stringr", 
  "tidytext",
  "stm",
  "quanteda",
  "tidyverse",
  "fields",
  "cluster",
  "rgl",
  "scales",
  "reshape2" # melt function
  ) -> package_names  
for(package_name in package_names) {
  if(!is.element(package_name, installed.packages()[,1])) {
     install.packages(package_name,
                      repos = "http://cran.mtu.edu/")
  }
  library(package_name, character.only=TRUE,
          quietly=TRUE,verbose=FALSE)
}
rm(list=c("package_name", "package_names")) # clean up the environment

options(scipen = 999)
```

# Reading in Data
```{r}
complete_df <- read.csv("~/COLLEGE/GRAD SCHOOL/Capstone/Restaurant_App/data/processed/complete_df.csv")
complete_df_app <- read.csv("~/COLLEGE/GRAD SCHOOL/Capstone/Restaurant_App/data/processed/complete_df_app.csv")
complete_df_app2 <- read.csv("~/COLLEGE/GRAD SCHOOL/Capstone/Restaurant_App/data/processed/complete_df_app2.csv")
```


# Oyster Price EDA
```{r}
# Filtering for food items that involve oysters
complete_df[grepl("Oyster|oyster", complete_df$Food),] -> oyster_df

# 51 unique restaurants that offer oyster items
unique(oyster_df$Restaurant)
str(oyster_df)

# mean oyster price is $12.74
mean(oyster_df$Price, na.rm = TRUE)

ggplot(data = oyster_df, mapping = aes(x = City, y = Price)) + 
  geom_point() +
  geom_hline(yintercept = mean(oyster_df$Price, na.rm = TRUE),
             size = 0.5, color = "red") + 
  labs(title = "Prices of Oyster Items by City",
       subtitle = "Mean of All Prices")
```


# General Price Menu EDA
```{r}
# mean price for entire menu data is $14.68
mean(complete_df$Price, na.rm = TRUE)

# filtering for mean menu price by city
complete_df %>%
  filter(City == "Boston" | City == "Charleston" | City == "Portland") %>%
  group_by(City) %>%
  summarise(Price = mean(Price, na.rm = TRUE)) -> menu_means

menu_means

ggplot(menu_means, mapping = aes(x = City, y = Price)) + 
  geom_col(color = "black", fill = "dark blue") + 
  ylim(0, 20) + 
  labs(title = "Mean Price of Menu Items by City") + 
  ylab("Price (dollars)")
```
Boston mean menu price is $15.60
Charleston mean menu price is $11.63
Portland mean menu price is $13.01


# Counting Number of Items per Restaurant
```{r}
# number of food items per restaurant (by address to account for franchises)
items_df <- aggregate(cbind(Items = complete_df$Food) ~ complete_df$Address,
                      FUN = function(x){NROW(x)})

# attaching corresponding restaurant name to address
items_df <- merge(items_df, complete_df_app, by.x = "complete_df$Address", by.y = "Address")

# drop column 3
items_df <- items_df[,-3]

# renaming column 1
colnames(items_df)[1] <- "Address"  

# largest to smallest number of items
items_df <- items_df %>%
  arrange(desc(Items))

items_df
```
Boston appears to have more items on their menu on average compared to the other cities (based on the sample of seafood restaurant data I have).


# Topic Modeling with R and Tidy Data Principles
```{r}
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)

# address instead of restaurant due to franchises
tm_df <- complete_df %>%
  mutate(Address = factor(Address, levels = unique(Address)))

# text pre-processing
tm_df$Food <- gsub("[^[:alnum:][:space:]]","", tm_df$Food)
tm_df$Food <- tolower(tm_df$Food)

tidy_tm_df <- tm_df %>%
  unnest_tokens(word, Food) %>%
  anti_join(stop_words) %>%
  filter(word != "chicken")

# let's check the most common words
tidy_tm_df %>%
  count(word, sort = TRUE)
# chicken is very common; let's remove as it will negatively impact our topic model

# removing more common words
common_words <- c("chicken", "fried", "shrimp", "salad")

tidy_tm_df2 <- tm_df %>%
  unnest_tokens(word, Food) %>%
  anti_join(stop_words) %>%
  filter(!word %in% common_words)

tidy_tm_df2 %>%
  count(word, sort = TRUE)
```
tidy_tm_df2 gets rid of some more common words as opposed to tidy_tm_df, which should be beneficial for the model going forward.


# Implement Topic Modeling
```{r}
library(stm)
library(quanteda)

menu_dfm <- tidy_tm_df %>% 
  count(Address, word, sort = TRUE) %>%
  cast_dfm(Address, word, n)

# train a topic model
topic_model <- stm(menu_dfm, K = 15, init.type = "Spectral")
summary(topic_model)

# topic model with reduced common words
menu_dfm2 <- tidy_tm_df2 %>%
  count(Address, word, sort = TRUE) %>%
  cast_dfm(Address, word, n)

topic_model2 <- stm(menu_dfm2, K = 15, init.type = "Spectral")
summary(topic_model2)

# topic_model2 with reduced number of topics
topic_model3 <- stm(menu_dfm2, K = 8, init.type = "Spectral")
summary(topic_model3)

# topic_model2 with more topics
topic_model4 <- stm(menu_dfm2, K = 25, init.type = "Spectral")
summary(topic_model4)

# topic_model2 with 5 topics
topic_model5 <- stm(menu_dfm2, K = 5, init.type = "Spectral")
summary(topic_model5)
```
Training a handful of different topic models to look at beta and gamma values, and hopefully make meaningful inferences between each of them.


# Tidying the Topic Model
```{r}
td_beta <- tidy(topic_model)
# beta matrix tells us what are the words that contribute to each topic

td_beta %>%
  group_by(topic) %>%
  top_n(10) %>%
  ungroup %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = topic)) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~topic, scales = "free") + 
  coord_flip()
# beta tells us which words contribute the most to which topic

td_gamma <- tidy(topic_model, matrix = "gamma",
                 document_names = rownames(menu_dfm))
# in this document, how much did this topic contribute to it

ggplot(td_gamma, aes(gamma, fill = as.factor(topic))) + 
  geom_histogram(show.legend = FALSE) + 
  facet_wrap(~topic, ncol = 3)
# gamma tells us how likely does this document belong to this topic

### let's use topic_model2 ###
td_beta2 <- tidy(topic_model2)

td_beta2 %>%
  group_by(topic) %>%
  top_n(10) %>%
  ungroup %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = topic)) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~topic, scales = "free") + 
  coord_flip()

td_gamma2 <- tidy(topic_model2, matrix = "gamma",
                  document_names = rownames(menu_dfm2))

ggplot(td_gamma2, aes(gamma, fill = as.factor(topic))) + 
  geom_histogram(show.legend = FALSE) + 
  facet_wrap(~topic, ncol = 3)

### let's use topic_model3 ###
td_beta3 <- tidy(topic_model3)

td_beta3 %>%
  group_by(topic) %>%
  top_n(10) %>%
  ungroup %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = topic)) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~topic, scales = "free") + 
  coord_flip()

td_gamma3 <- tidy(topic_model3, matrix = "gamma",
                  document_names = rownames(menu_dfm2))

ggplot(td_gamma3, aes(gamma, fill = as.factor(topic))) + 
  geom_histogram(show.legend = FALSE) + 
  facet_wrap(~topic, ncol = 3)

### let's use topic_model4 ###
td_beta4 <- tidy(topic_model4)

td_beta4 %>%
  group_by(topic) %>%
  top_n(10) %>%
  ungroup %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = topic)) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~topic, scales = "free") + 
  coord_flip()

td_gamma4 <- tidy(topic_model4, matrix = "gamma",
                  document_names = rownames(menu_dfm2))

ggplot(td_gamma4, aes(gamma, fill = as.factor(topic))) + 
  geom_histogram(show.legend = FALSE) + 
  facet_wrap(~topic, ncol = 3)

### let's use topic_model5 ###
td_beta5 <- tidy(topic_model5)

td_beta5 %>%
  group_by(topic) %>%
  top_n(10) %>%
  ungroup %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = topic)) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~topic, scales = "free") + 
  coord_flip()

td_gamma5 <- tidy(topic_model5, matrix = "gamma",
                  document_names = rownames(menu_dfm2))

ggplot(td_gamma5, aes(gamma, fill = as.factor(topic))) + 
  geom_histogram(show.legend = FALSE) + 
  facet_wrap(~topic, ncol = 3)
```
Do the restaurants seem to better fit the topics they're being assigned to in topic_model2? In some restaurants I've looked at, yes. Could be attributed to reducing more of the common words that appear (menu_dfm2).

Try reducing number of topics in next model (topic_model3):

Noticing Seafood, New England, and Steak as cuisine types for topic 3. Food items that contribute the most to topic 3 (crab, grilled, lobster).

Topic 5 consists of mainly these cuisines: 
Pizza, Sandwiches, Seafood, Chicken, Pasta, Wraps, Subs	
American
Italian, Pizza, Seafood, Pasta

Food items that contribute the most to topic 5 (pizza, steak, cheese). Restaurants with high gamma value in this topic seem to fit this topic.

topic_model5:
Topic 5 features lots of Chinese/Asian/Japanese seafood restaurants.

Topic 4 is crab oriented.

Topic 3 has 29 restaurants with a gamma value greater than 0.90.


# Re-shaping td_gamma for Visualization
```{r}
# before reshaping; looking at each topic
ggplot(td_gamma, aes(x = topic, y = gamma)) + 
  geom_point()

# boxplot for topic_model5
td_gamma5$topic <- as.factor(td_gamma5$topic)

ggplot(td_gamma5, aes(x = topic, y = gamma, fill = topic)) + 
  geom_boxplot(outlier.color = "black", outlier.shape = 19,
               outlier.size = 2)

# boxplot for topic_model (original)
td_gamma$topic <- as.factor(td_gamma$topic)

ggplot(td_gamma, aes(x = topic, y = gamma, fill = topic)) + 
  geom_boxplot(outlier.color = "black", outlier.shape = 19,
               outlier.size = 2) + 
  facet_wrap(~topic, ncol = 5)

# converting to a wide data frame
td_gamma_wide <- spread(td_gamma, key = topic, value = gamma)

# gamma for each topic adds up to 1 per restaurant (as they should)
rowSums(td_gamma_wide[, -1])

# topic 3 graph for original topic_model
ggplot(td_gamma_wide, aes(x = document, y = `3`)) + 
  geom_point() + 
  geom_text(aes(label = ifelse(`3` >= 0.50, document, ""), hjust = 1, vjust = 1)) + 
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) + 
  labs(x = "Restaurant",
       y = "Gamma",
       title = "Gamma Values of Restaurants Assigned to Topic 3",
       subtitle = "According to the Original topic_model")

# same thing but for topic_model5
td_gamma_wide5 <- spread(td_gamma5, key = topic, value = gamma)

# gamma for each topic adds up to 1 per restaurant (as they should)
rowSums(td_gamma_wide5[, -1])

# topic 3 graph
ggplot(td_gamma_wide5, aes(x = document, y = `3`)) + 
  geom_point() + 
  geom_text(aes(label = ifelse(`3` >= 0.50, document, ""), hjust = 1, vjust = 1)) + 
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) + 
  labs(x = "Restaurant",
       y = "Gamma",
       title = "Gamma Values of Restaurants Assigned to Topic 3",
       subtitle = "According to topic_model5")
```


# Creating a Euclidean Distance Matrix
```{r}
library(tidyverse)
column_to_rownames(td_gamma_wide, var = "document") -> td_gamma_test

library(fields)
distmatrix <- rdist(td_gamma_test, td_gamma_test)

# kmeans
fit <- kmeans(distmatrix, 15)

# cluster plot
library(cluster)
clusplot(td_gamma_test, fit$cluster,
         color = TRUE, shade = TRUE,
         labels = 2, lines = 0)

# kmeans 2nd fit
fit2 <- kmeans(distmatrix, 5)

# cluster plot (2nd fit)
clusplot(td_gamma_test, fit2$cluster,
         color = TRUE, shade = TRUE,
         labels = 2, lines = 0)

library(rgl)
# include number of clusters in new dataset
newdf <- data.frame(distmatrix, K = fit$cluster)
pcdf <- princomp(distmatrix, cor = TRUE, score = TRUE)
summary(pcdf)

plot(pcdf, type = "lines")

plot3d(pcdf$scores, col = newdf$K)
```
The first kmeans fit uses 15 topics. When clustered, it's a bit straining to look through, so I also made a kmeans that uses only 5 topics.
Note: this originates from our original 'topic_model', which used 'menu_dfm'. 'menu_dfm' did not remove as many common words as 'menu_dfm2', so you will want to test this out on a topic model that uses 'menu_dfm2' as well.


# Hierarchical Clustering
```{r}
# hierarchical clustering
di <- dist(menu_dfm2, method = "euclidean")
tree <- hclust(di, method = "ward.D")
# using restaurant name as label; complete_df_app2 accounts for duped restaurant names with part of address added
plot(tree, labels = complete_df_app2$Restaurant)
rect.hclust(tree, k = 3, border = "red")
```
Franchises (same menu) are clustered together, which is a good sign. Double-checked the distmatrix and their Euclidean distances are properly valued (e.g. franchises with the same menu have distances of 0).


# Adding Restaurant Names to the Distance Matrix
```{r}
td_gamma_wideNames <- td_gamma_wide
# changing column name to match with complete_df_app
colnames(td_gamma_wideNames)[1] <- "Address"

# now we have the correct order of names to add to the distance matrix
td_euc <- merge(td_gamma_wideNames, complete_df_app2, by = "Address")

# making a dupe of it
distmatrix2 <- distmatrix

# copying these restaurant names
dput(td_euc$Restaurant)

# changing the column names to the corresponding restaurant name
colnames(distmatrix2) <- c("Ostra", "Legal C Bar", "Aura", "Legal Osteria", "The Boathouse 10", 
"Hank's Seafood Restaurant", "Legal Sea Foods 100", "Harborside Grill and Patio", 
"Kipo's", "Susan's Fish N Chips", "Anson", "Stockyard", "Boston Chops", 
"Morse Fish Company", "Charleston Crab House 145", "Captain Marden's Seafoods", 
"Walter's", "No Name Restaurant", "James Hook & Co Lobsters", 
"Blackstone Grill", "Pearlz Oyster Bar 153", "Long John Silver's", 
"Central Wharf Co.", "Cypress Restaurant", "Blossom", "Himalayan Bistro", 
"Phu-Ket Thai Restaurant", "Fleet Landing", "The Tap", "The Crab Shack 1901", 
"Captain D's 1936", "The Daily Catch 2", "The Upper Crust Pizzeria", 
"Tia's", "Amen Street Fish & Rawbar", "Hyman's Seafood", "The Falmouth Sea Grill", 
"75 on Liberty Wharf", "LTK", "The Lobster Shack", "Alfredo's Italian Kitchen", 
"GaGa Seafood", "DiMillo's Floating Restaurant & Marina", "Legal Sea Foods 255", 
"Legal Sea Foods 26", "Off the Boat", "Four Winds Seafood Grille", 
"East Ocean City", "Saigon Seafood Restaurant", "Legal Harborside", 
"Brother's Crawfish", "Waterline", "Yankee Lobster", "Supreme House of Pizza", 
"The Daily Catch 323", "Street & Co.", "Durgin Park", "A W Shuck's Seafood", 
"Captain Fishbones", "The Boathouse 38", "Weathervane Seafood Restaurant", 
"Row 34", "Coast", "Oceanaire", "Captain D's 4008", "Arboretum Pizza Grill", 
"Rincon Limeno Restaurant", "Charleston Crab House 41", "Union Oyster House", 
"Thai North Catering", "Fish", "Bubor Cha Cha", "Nick's House of Pizza & Seafood", 
"China Bo ($1 Chinese Rice/#1 Chinese Food)", "Fire House Seafood", 
"New Jumbo Seafood", "J's Oyster", "Jasper White's Summer Shack", 
"Island Creek Oyster Bar", "Locklear's Fine Seafood", "Miel", 
"Captain D's 5130", "Hoshiya Sushi", "The Boathouse 549", "B&G Oysters", 
"Legal Crossing", "Chart House", "Neptune Oyster", "Captain D's 6326", 
"Lorenz Island Kuisine", "New Golden Gate", "24 Hour Pizza Delivery", 
"75 Chestnut", "Luke's Lobster", "Royal Roast Beef and Seafood", 
"Atlantic Fish Company", "L'Espalier", "Abe & Louie's Steakhouse", 
"Boston Sail Loft", "Crab Shack", "Chau Chow City", "Huff's Seafood", 
"The Crab Shack 8486", "KO Pies", "Barking Crab", "Pearlz Oyster Bar 9", 
"Towne Stove and Spirits", "Gilbert's Chowder House", "Old Port Sea Grill & Raw Bar", 
"Bubba Gump Shrimp Co.", "McCormick & Schmick's")

# likewise for row names
rownames(distmatrix2) <- c("Ostra", "Legal C Bar", "Aura", "Legal Osteria", "The Boathouse 10", 
"Hank's Seafood Restaurant", "Legal Sea Foods 100", "Harborside Grill and Patio", 
"Kipo's", "Susan's Fish N Chips", "Anson", "Stockyard", "Boston Chops", 
"Morse Fish Company", "Charleston Crab House 145", "Captain Marden's Seafoods", 
"Walter's", "No Name Restaurant", "James Hook & Co Lobsters", 
"Blackstone Grill", "Pearlz Oyster Bar 153", "Long John Silver's", 
"Central Wharf Co.", "Cypress Restaurant", "Blossom", "Himalayan Bistro", 
"Phu-Ket Thai Restaurant", "Fleet Landing", "The Tap", "The Crab Shack 1901", 
"Captain D's 1936", "The Daily Catch 2", "The Upper Crust Pizzeria", 
"Tia's", "Amen Street Fish & Rawbar", "Hyman's Seafood", "The Falmouth Sea Grill", 
"75 on Liberty Wharf", "LTK", "The Lobster Shack", "Alfredo's Italian Kitchen", 
"GaGa Seafood", "DiMillo's Floating Restaurant & Marina", "Legal Sea Foods 255", 
"Legal Sea Foods 26", "Off the Boat", "Four Winds Seafood Grille", 
"East Ocean City", "Saigon Seafood Restaurant", "Legal Harborside", 
"Brother's Crawfish", "Waterline", "Yankee Lobster", "Supreme House of Pizza", 
"The Daily Catch 323", "Street & Co.", "Durgin Park", "A W Shuck's Seafood", 
"Captain Fishbones", "The Boathouse 38", "Weathervane Seafood Restaurant", 
"Row 34", "Coast", "Oceanaire", "Captain D's 4008", "Arboretum Pizza Grill", 
"Rincon Limeno Restaurant", "Charleston Crab House 41", "Union Oyster House", 
"Thai North Catering", "Fish", "Bubor Cha Cha", "Nick's House of Pizza & Seafood", 
"China Bo ($1 Chinese Rice/#1 Chinese Food)", "Fire House Seafood", 
"New Jumbo Seafood", "J's Oyster", "Jasper White's Summer Shack", 
"Island Creek Oyster Bar", "Locklear's Fine Seafood", "Miel", 
"Captain D's 5130", "Hoshiya Sushi", "The Boathouse 549", "B&G Oysters", 
"Legal Crossing", "Chart House", "Neptune Oyster", "Captain D's 6326", 
"Lorenz Island Kuisine", "New Golden Gate", "24 Hour Pizza Delivery", 
"75 Chestnut", "Luke's Lobster", "Royal Roast Beef and Seafood", 
"Atlantic Fish Company", "L'Espalier", "Abe & Louie's Steakhouse", 
"Boston Sail Loft", "Crab Shack", "Chau Chow City", "Huff's Seafood", 
"The Crab Shack 8486", "KO Pies", "Barking Crab", "Pearlz Oyster Bar 9", 
"Towne Stove and Spirits", "Gilbert's Chowder House", "Old Port Sea Grill & Raw Bar", 
"Bubba Gump Shrimp Co.", "McCormick & Schmick's")
```
Now that we have our distance matrix properly labeled with the corresponding restaurant names, we can move on to making a data frame featuring the lowest Euclidean distances.


# Creating a Data Frame with the Lowest Euclidean Distances
```{r}
distmatrix_df <- as.data.frame(distmatrix2)

# now we need to remove the zeroes, as the franchises are identical to each other (we want different, but similar restaurants) ## dont do yet

# need to get df into a long format
longdf <- tibble::rownames_to_column(distmatrix_df, "Similar_Restaurant")
longdf <- melt(longdf)
# changing the column names
colnames(longdf)[2] <- "Restaurant"
colnames(longdf)[3] <- "Distance"

longdf <- merge(longdf, complete_df_app2, by = "Restaurant")

# getting rid of unnecessary columns
longdf <- longdf[,-(4:5)]
# arranging by restaurant and lowest distance
# only care about distances greater than 0 (otherwise we'll get the restaurant being observed as a similar restaurant) 
longdf <- longdf %>% 
  arrange(Restaurant, Distance) %>% 
  filter(Distance > 0)

# getting the top 5 lowest distances
similar <- longdf %>% 
  group_by(Restaurant) %>% 
  top_n(-5, Distance)

# for now... pasting list of the 5 similar restaurant names into a column with the respective restaurant
similar <- aggregate(similar$Similar_Restaurant, list(similar$Restaurant), paste, collapse = ", ")

# renaming column
colnames(similar)[1] <- "Restaurant"

# merging the similar restaurants with complete_df_app2
sim_rest_df <- merge(similar, complete_df_app2, by = "Restaurant")

# getting rid of unnecessary columns
sim_rest_df <- sim_rest_df[,-(3:4)]

# renaming column
colnames(sim_rest_df)[2] <- "Similar_Restaurant"

#saveRDS(sim_rest_df, "~/COLLEGE/GRAD SCHOOL/Capstone/Restaurant_App/data/processed/sim_rest_df")
 
#write.csv(sim_rest_df, "~/COLLEGE/GRAD SCHOOL/Capstone/Restaurant_App/data/processed/sim_rest_df.csv")

# now we can put this into the app (replace complete_df_app with sim_rest_df)
```









## Prior Use

# Latent Dirichlet Allocation (LDA)
```{r}
#https://rpubs.com/Junny31/318845
library(tm)
library(SnowballC)

# creating a corpus
text_corpus <- VCorpus(VectorSource(complete_df$Food))

print(text_corpus)

# text preparation
text_corpus_clean <- tm_map(text_corpus, content_transformer(tolower)) %>%
  tm_map(., stemDocument) %>%
  tm_map(., removeNumbers) %>%
  tm_map(., removeWords, stopwords()) %>%
  tm_map(., removePunctuation) %>%
  tm_map(., stripWhitespace)


library(wordcloud)
# plot words with a minimum frequency of 20
wordcloud(text_corpus_clean, min.freq = 20, random.order = FALSE,
          colors=brewer.pal(8, "Dark2"))

# model training
text_dtm <- DocumentTermMatrix(text_corpus_clean)
text_dtm

findFreqTerms(text_dtm, lowfreq = 20)

# each row of the input matrix needs to contain at least one non-zero entry
ui = unique(text_dtm$i)
text_dtm.new = text_dtm[ui,]

library(topicmodels)

text_lda <- LDA(text_dtm.new, k = 30, method = "VEM", control = NULL)
text_lda

# model evaluation
library(tidytext)

text_topics <- tidy(text_lda, matrix = "beta")

text_topics

library(ggplot2)
library(dplyr)

text_top_terms <- text_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

text_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```


# More with LDA
```{r}
# https://towardsdatascience.com/beginners-guide-to-lda-topic-modelling-with-r-e57a5a8e7a25
library(textmineR)

# just want the food items
foodItems <- as.data.frame(complete_df[,5])
colnames(foodItems)[1] <- "Food"

# text pre-processing
foodItems <- as.data.frame(gsub("[^[:alnum:][:space:]]","", foodItems$Food))
colnames(foodItems)[1] <- "Food"

# create document term matrix
dtm <- CreateDtm(foodItems$Food,
                 ngram_window = c(1, 2))

# explore the basic frequency
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq, doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)

# Eliminate words appearing less than 2 times or in more than half of the documents
vocabulary <- tf$term[tf$term_freq > 1 & tf$doc_freq < nrow(dtm) / 2]

dtm = dtm

# model building
model <- FitLdaModel(dtm = dtm, k = 8, iterations = 100)

coherence_df <- as.data.frame(model$coherence)

coherence_df$id <- row.names(coherence_df)

# plotting coherence score
ggplot(coherence_df, aes(x = id, y = model$coherence)) +
  geom_point() +
  geom_line(group = 1)+
  ggtitle("Best Topic by Coherence Score") + theme_minimal() +
  ylab("Coherence") + 
  xlab("k") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
# topic 2 has the best coherence score

# top 20 terms to best describe the topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 20)
top20_wide <- as.data.frame(model$top_terms)

# hierarchical clustering of the topics
model$topic_linguistic_dist <- CalcHellingerDist(model$phi)

model$hclust <- hclust(as.dist(model$topic_linguistic_dist), "ward.D")
model$hclust$labels <- paste(model$hclust$labels, model$labels[,1])

plot(model$hclust)
```

k = 2 yields the highest coherence score. In other words, the model believes that the words in topic 2 are most semantically similar among the 20 topics.


# Text Analysis with quanteda
```{r}
#https://data.library.virginia.edu/a-beginners-guide-to-text-analysis-with-quanteda/
library(quanteda)

# just want the food items and an id column
foodItems <- as.data.frame(complete_df[,c(2,5)])
colnames(foodItems)[1] <- "id"

# text pre-processing
foodItems$Food <- gsub("[^[:alnum:][:space:]]","", foodItems$Food)

# creating a corpus
doc.corpus <- corpus(foodItems$Food)
summary(doc.corpus)

# cleaning and creating tokens
doc.tokens <- tokens(doc.corpus)
doc.tokens <- tokens(doc.tokens, remove_punct = TRUE, 
                     remove_numbers = TRUE)
# removing stop words
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
# stem (reduce each word down to its base form) the tokens
doc.tokens <- tokens_wordstem(doc.tokens)
# convert to lowercase
doc.tokens <- tokens_tolower(doc.tokens)

# converting to a document feature matrix
doc.dfm <- dfm(doc.corpus, remove_numbers = TRUE,
               stem = TRUE,
               remove = stopwords("english"))
# use previously created tokenized object as dfm to be safe
doc.dfm.final <- dfm(doc.tokens)

# look at certain words
View(head(kwic(doc.tokens, "shrimp", window = 3)))

# looking at the top 5 words
topfeatures(doc.dfm.final, 5)

# top 20 words
list_top20 <- rownames(as.data.frame(topfeatures(doc.dfm.final, 20)))

View(kwic(doc.tokens, list_top20, window = 3))

corpus_dictionary <- as.data.frame(kwic(doc.corpus, list_top20))

# top 50 words
topfeatures(doc.dfm.final, 50)
list_top50 <- rownames(as.data.frame(topfeatures(doc.dfm.final, 50)))

dictionary_50 <- kwic(doc.tokens, list_top50, window = 3)

View(kwic(doc.tokens, list_top50, window = 3))
```


# https://github.com/bmschmidt/wordVectors/blob/master/vignettes/introduction.Rmd
```{r}
if (!require(wordVectors)) {
  if (!(require(devtools))) {
    install.packages("devtools")
  }
  devtools::install_github("bmschmidt/wordVectors")
}

library(wordVectors)
library(magrittr)

if (!file.exists("cookbooks.zip")) {
  download.file("http://archive.lib.msu.edu/dinfo/feedingamerica/cookbook_text.zip","cookbooks.zip")
}
unzip("cookbooks.zip",exdir="cookbooks")

if (!file.exists("cookbooks.txt")) prep_word2vec(origin="cookbooks",destination="cookbooks.txt",lowercase=T,bundle_ngrams=2)

# training
if (!file.exists("cookbook_vectors.bin")) {model = train_word2vec("cookbooks.txt","cookbook_vectors.bin",vectors=200,threads=4,window=12,iter=5,negative_samples=0)} else model = read.vectors("cookbook_vectors.bin")
```


# applying to my data
```{r}
library(wordVectors)
# reading in the model
model <- readRDS("./models/word2vec_model.rds")

test <- complete_df

# text pre-processing
test$Food <- gsub("[^[:alnum:][:space:]]","", test$Food)
test$Food <- tolower(test$Food)

test_df <- test[5]

# saving as a txt file
#write.table(test_df,"food.txt",sep="\t",row.names=FALSE)

# training
model <- train_word2vec("./food.txt")
#saveRDS(model, file = "~/COLLEGE/GRAD SCHOOL/Capstone/Restaurant_App/code/EDA/models/word2vec_model.rds")

# similarity searches
model %>% 
  closest_to(model[[c("fish","salmon","trout","shad","flounder","carp","roe","eels")]], 50)

some_fish = closest_to(model,model[[c("fish","salmon","trout","shad","flounder","carp","roe","eels")]],150)
fishy = model[[some_fish$word,average=F]]
plot(fishy,method="pca")

# clustering
clustering = kmeans(model, centers = 150, iter.max = 40)

sapply(sample(1:150,10), function(n) {
  names(clustering$cluster[clustering$cluster==n][1:10])
})

# take the 20 words closest to each of four different kinds of words
ingredients = c("madeira","beef","saucepan","carrots")
term_set = lapply(ingredients, 
       function(ingredient) {
          nearest_words = model %>% closest_to(model[[ingredient]],20)
          nearest_words$word
        }) %>% unlist
subset = model[[term_set,average=F]]
subset %>%
  cosineDist(subset) %>% 
  as.dist %>%
  hclust %>%
  plot

# # find 20 words most similar to "sweet" and "salty"
# tastes = model[[c("sweet","salty"),average=F]]
# # model[1:100,] here restricts to the 1000 most common words in the set.
# sweet_and_saltiness = model[1:1000,] %>% cosineSimilarity(tastes)
# # Filter to the top 20 sweet or salty.
# sweet_and_saltiness = sweet_and_saltiness[
#   rank(-sweet_and_saltiness[,1])<20 |
#   rank(-sweet_and_saltiness[,2])<20,
#   ]
# plot(sweet_and_saltiness,type='n')
# text(sweet_and_saltiness,labels=rownames(sweet_and_saltiness))

# catchall reduction: TSNE
library(tsne)

plot(model, perplexity = 7)
```
read in a previously trained model with `model =  read.vectors("cookbook_vectors.bin")`


# trying to cluster the restaurants
```{r}
library(cluster)
library(fpc)

kmeans(menu_dfm2, centers = 5) -> test
plotcluster(menu_dfm2, test$cluster)

### document clustering ###
library(textmineR)

# get IDF vector as a data frame
tf_mat <- TermDocFreq(menu_dfm2)

# TF-IDF and cosine similarity
tfidf <- t(menu_dfm2[ , tf_mat$term]) * tf_mat$idf

# calculate cosine similarity
csim <- tfidf / sqrt(rowSums(tfidf * tfidf))

csim <- csim %*% t(csim)

# convert cosine similarity to distance by subtracting it from 1
cdist <- as.dist(1 - csim)

# clustering
hc <- hclust(cdist, "ward.D")
plot(hc)

```

